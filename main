main

def classify_device(user_agent):
    if pd.isna(user_agent):
        return 'Other/Bot'
    ua = user_agent.lower()
    if 'windows nt' in ua:
        return 'Windows'
    elif 'macintosh' in ua:
        return 'MacOs'
    elif 'ipad' in ua or 'iphone' in ua:
        return 'iOS Mobile'
    elif 'android' in ua:
        return 'Android Mobile'
    else:
        return 'Other/Bot'

def get_latest(df, log_type, cols):
    subset = df[df['Log_Type'] == log_type].copy()
    subset = subset.sort_values(['CloudID', 'EventTimestamp'], ascending=[True, False])
    latest = subset.groupby('CloudID').first().reset_index()
    return latest[['CloudID'] + cols]

df_email = get_latest(df_events, 'UpdateUserEmail', ['JSON_DATA'])
df_email['LatestEmail'] = df_email['JSON_DATA'].apply(lambda x: x.get('email') if isinstance(x, dict) else None)

df_name = get_latest(df_events, 'UpdateUserName', ['JSON_DATA'])
df_name['FirstName'] = df_name['JSON_DATA'].apply(lambda x: x.get('firstName') if isinstance(x, dict) else None)
df_name['LastName'] = df_name['JSON_DATA'].apply(lambda x: x.get('lastName') if isinstance(x, dict) else None)

df_location = get_latest(df_events, 'UpdateUserLocation', ['JSON_DATA'])
df_location['City'] = df_location['JSON_DATA'].apply(lambda x: x.get('city') if isinstance(x, dict) else None)
df_location['PostalCode'] = df_location['JSON_DATA'].apply(lambda x: x.get('postalCode') if isinstance(x, dict) else None)





df_digital = df_events[df_events['Log_Type'].isin(['CreateRoute', 'CheckPoint', 'NewCookie'])].copy()

def map_digital_product(sub_code):
    product_mapping = {
        'PS01': 'Ticket Hub Account',
        'PS02': 'Audio Library',
        'FIT01': 'Music Subscription',
        'SDR01': 'Video Stream',
        'SDR01': 'Podcast Library',
        'PFG01': 'Digital Studio Access'
    }
    return product_mapping.get(sub_code, 'Uncategorized')

df_digital['RequestedProductCode'] = df_digital.apply(
    lambda x: x['JSON_DATA'].get('requestedProductCode')
    if x['Log_Type'] == 'NewCookie'
    else x['JSON_DATA'].get('ProductCode')
    if x['Log_Type'] == 'CheckPoint'
    else x['ITEM_DETAILS'].get('selectedProductType', ['Undefined'])[0]
    if isinstance(x['ITEM_DETAILS'], dict)
    else 'Undefined',
    axis=1
)

df_digital['ProductDisplayName'] = df_digital['JSON_DATA'].apply(
    lambda x: map_digital_product(x.get('requestedSubProductCode')) if isinstance(x, dict) else 'Uncategorized'
)

df_first_digital = (
    df_digital.groupby('CloudID', as_index=False)
    .apply(lambda g: g.loc[g['EventTimestamp'].idxmin(), ['CloudID', 'RequestedProductCode', 'ProductDisplayName']])
    .reset_index(drop=True)
)
df_first_digital.columns = ['CloudID', 'FinalProductCode', 'FinalProductName']



# =============================
# USER ROUTE CLASSIFICATION
# =============================

new_cookie = df_events[df_events['Log_Type'] == 'NewCookie'][['CloudID']].drop_duplicates()
new_cookie['Route'] = 'New Cookie'

existing_cookie = df_events[
    (df_events['Log_Type'].isin(['CheckPoint', 'LastField'])) &
    (
        df_events['JSON_DATA'].apply(lambda x: isinstance(x, dict) and (
            x.get('isReturningCookie') == True or
            'ExistingCookieAuth' in str(x.get('flowStep', ''))
        ))
    )
][['CloudID']].drop_duplicates()
existing_cookie['Route'] = 'Existing Cookie'

df_route = pd.merge(existing_cookie, new_cookie, on='CloudID', how='outer', suffixes=('_existing', '_new'))

def route_category(row):
    if pd.notna(row['Route_existing']) and pd.notna(row['Route_new']):
        return 'Returning Cookie (New Access)'
    elif pd.notna(row['Route_new']):
        return 'New Cookie'
    elif pd.notna(row['Route_existing']):
        return 'Returning Cookie'
    else:
        return 'Guest'

df_route['ClientRouteCategory'] = df_route.apply(route_category, axis=1)
df_route = df_route[['CloudID', 'ClientRouteCategory']]

df_final = (
    df_event_context
    .merge(df_first_digital, on='CloudID', how='left')
    .merge(df_route, on='CloudID', how='left')
    .merge(df_email[['CloudID', 'LatestEmail']], on='CloudID', how='left')
    .merge(df_name[['CloudID', 'FirstName', 'LastName']], on='CloudID', how='left')
    .merge(df_location[['CloudID', 'City', 'PostalCode']], on='CloudID', how='left')
)

# Add placeholder analytics columns
df_final['EngagementScore'] = 0
df_final['FraudAlertFlag'] = 0
df_final['HoldFlag'] = 0




data = pd.DataFrame({
    'timestamp': pd.date_range(start='2025-10-01', periods=200, freq='6H'),
    'log_type': ['download', 'view', 'download', 'stream'] * 50,
    'event': ['open_app', 'play_audio', 'add_to_library', 'close_app'] * 50,
    'cookie_id': ['cookie_new1', 'cookie_new2', 'cookie_exist1', 'cookie_exist2'] * 50,
    'route': ['/music', '/video', '/podcast', '/ticket'] * 50,
    'sub_product_code': ['AUD01', 'VID01', 'POD01', 'TKT01'] * 50
})

# --- Rename concept mapping ---
Log_type = 'log_type'
event = 'event'
cookieID = 'cookie_id'
CloudID = 'cloud_id'
Route = 'route'
clientRouteCategory = 'client_route_category'
ITEMS_OBJECT = 'digital_asset'

# Replace product mapping with digital services
product_mapping = {
    'TKT01': 'Ticket Hub Account',
    'AUD01': 'Audio Library',
    'MUS01': 'Music Subscription',
    'VID01': 'Video Stream',
    'POD01': 'Podcast Library'
}

# --- Add a derived field: "new_cookie" flag ---
data['is_new_cookie'] = data[cookieID].apply(lambda x: 'new' in x.lower())

# --- Extract date for daily aggregation ---


data['date'] = pd.to_datetime(data['timestamp']).dt.date

# --- Apply mapping ---
data['product_name'] = data['sub_product_code'].map(product_mapping)

# --- Filter only download events ---
downloads = data[data[Log_type] == 'download']

# --- Aggregate: count CloudIDs (cookies) per day, per sub_product_code, per new/existing cookie ---
agg = (
    downloads.groupby(['date', 'sub_product_code', 'is_new_cookie'])
    [cookieID].nunique()  # count unique cookie IDs = CloudIDs
    .reset_index(name='unique_cloudid_count')
)

# --- Separate for plotting ---
new_cookie_df = agg[agg['is_new_cookie'] == True]
existing_cookie_df = agg[agg['is_new_cookie'] == False]

# --- Plot ---
plt.figure(figsize=(10, 6))

for code in agg['sub_product_code'].unique():
    new_cookie_series = new_cookie_df[new_cookie_df['sub_product_code'] == code]
    existing_cookie_series = existing_cookie_df[existing_cookie_df['sub_product_code'] == code]
    
    plt.plot(new_cookie_series['date'], new_cookie_series['unique_cloudid_count'], 
             label=f'New Cookie - {code}', linestyle='--', marker='o')
    plt.plot(existing_cookie_series['date'], existing_cookie_series['unique_cloudid_count'], 
             label=f'Existing Cookie - {code}', linestyle='-', marker='x')

plt.title('Daily Downloads by Sub Product and Cookie Type')
plt.xlabel('Date')
plt.ylabel('Unique CloudID Count')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()




# --- Product mapping (digital services) ---
product_mapping = {
    'TKT01': 'Ticket Hub Account',
    'AUD01': 'Audio Library',
    'MUS01': 'Music Subscription',
    'VID01': 'Video Stream',
    'POD01': 'Podcast Library'
}

# --- Add flag for new vs existing cookies ---
data['is_new_cookie'] = data[cookieID].apply(lambda x: 'new' in x.lower())

# --- Extract date ---
data['date'] = pd.to_datetime(data['timestamp']).dt.date

# --- Map product names ---
data['product_name'] = data['sub_product_code'].map(product_mapping)

# --- Filter for download events only ---
downloads = data[data[Log_type] == 'download']

# --- Aggregate: count unique cookie IDs (CloudIDs) per day, product, and cookie type ---
agg = (
    downloads.groupby(['date', 'sub_product_code', 'is_new_cookie'])
    [cookieID].nunique()
    .reset_index(name='unique_cloudid_count')
)

# --- Add 7-day rolling average ---
agg = agg.sort_values('date')
agg['rolling_avg'] = (
    agg.groupby(['sub_product_code', 'is_new_cookie'])['unique_cloudid_count']
    .transform(lambda x: x.rolling(window=7, min_periods=1).mean())
)

# --- Plot: One chart per product, with new vs existing lines ---
unique_products = agg['sub_product_code'].unique()
num_products = len(unique_products)
fig, axes = plt.subplots(num_products, 1, figsize=(10, 5 * num_products), sharex=True)

if num_products == 1:
    axes = [axes]

for ax, code in zip(axes, unique_products):
    subset = agg[agg['sub_product_code'] == code]
    new_cookie = subset[subset['is_new_cookie'] == True]
    existing_cookie = subset[subset['is_new_cookie'] == False]
    
    ax.plot(new_cookie['date'], new_cookie['rolling_avg'], label='New Cookie', linestyle='--', marker='o')
    ax.plot(existing_cookie['date'], existing_cookie['rolling_avg'], label='Existing Cookie', linestyle='-', marker='x')
    
    ax.set_title(f"{product_mapping.get(code, code)} - 7-Day Rolling Average")
    ax.set_ylabel("Unique CloudID Count")
    ax.legend()
    ax.grid(True)

plt.xlabel("Date")
plt.suptitle("Daily Download Trends by Digital Product and Cookie Type (7-Day Rolling Average)", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()




